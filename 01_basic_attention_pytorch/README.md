**Basic attention implementation using pytorch**
Reference :
1) http://peterbloem.nl/blog/transformers
2) https://pytorch.org/tutorials/beginner/transformer_tutorial.html
3) Pytorch basics - https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html

**Attention**

To produce output vector ğ²i, the self attention operation simply takes a weighted average over all the input vectors.
ğ²i=âˆ‘jwijğ±j
